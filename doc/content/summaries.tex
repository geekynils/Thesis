\section{Summaries of Related Works}
\label{sec:summary}

\subsection{Nvidia's Particles Demo}
\label{sec:summaryNvParticles}
The demo \cite{cudaparticles} shows how to efficiently implement a simple particle system in CUDA. It includes interactions between neighboring particles using a uniform grid. Nvidia suggests that the demo is used as a framework upon which more complicated particle interactions such as smoothed particle hydrodynamics (SPH) or soft body simulations can be built.

There are three main steps in the simulation: Integration, building the grid data structure and processing collisions. The visualization is done by directly using OpenGL together with CUDA. By default the system simulates 16384 particles in real time. On a GTX 470 it was possible however to simulate around $10^5$ particles in real time (with a framerate of about 30 frames per second). The particle-particle interactions are simplified using spatial subdivision. Because the interaction force drops off with distance, the force for a given particle is computed by only comparing it with particles within a certain radius. Particles
farther away are not considered.

The integration of the particle data (position and velocity) is simply done using the Euler method. The grid consists of cubes with a side length of the same size as the particle's diameter (all particles are of the same diameter too). Using such a grid greatly simplifies further computations: Each particle can only cover 8 cells at most and at most four particles can theoretically reside in one cell. The grid used is called loose, because each particle can only be assigned to one cell, even though it might overlap cell boundaries. Because the particle can overlap several cells, when processing collisions all the neighbour cells must be examined ($3\cdot3\cdot3=27$).

There are two methods of how the grid can be built: The first one uses atomic operations. Two arrays are used: One which stores the number of particles in each cell and one which stores the particle indices for each cell. Both are rebuilt at every timeframe. The kernel runs with one thread per particle. The second method builds the grid using sorting and was designed for older GPUs which do not support atomic operations. However in the current version of the particles demo the atomic version was removed, because the version which uses 
sorting performs better.

Further the paper mentions that binding the global memory arrays to textures improved performance by 45\% because texture reads are cached. These arrays are used to fetch the particles' position and veolcity which is typically non-coalesced.

\subsection{The Lagrangian Particles in the EM Driven Turbulent Flow with Fine Mesh}
This paper \cite{solidParticle11} presents a simulation in which the flow of induction in a crucible furnace (ICF) is simulated using the \verb+solidParticle+ library from OpenFOAM. The particles are used to represent the metal flow inside the ICF while the (turbulent) electromagnetic field is represented by the Eulerian phase. The particles are moved by a one way coupling where only the vector field of the cell in which the particle currently remains is taken into account. An interpolation of the velocity of the surrounding cells to the actual position of the particle is not used. The authors mention that the collision with the wall did not work correctly when the particles diameter is larger than the cell side, they therefore had to fix the code in order to get the simulation working. The authors finally conclude that the industrial observations correspond to the observations they made with their simulation.


\subsection{Complex Chemistry Modeling of Diesel Spray Combustion}

This PhD thesis \cite{nordin00} is about simulating Diesel sprays and combustion. It starts with explaining why it is necessary to treat the liquid phase (Diesel in this case) in a Lagrangian way: In a Diesel engine the fuel is injected through a spray which has a diameter on the order of 0.1 mm, with a velocity of 200-400 m/s. The subsequent ignition and the combustion require length scales, which are even smaller. Using the finite volume method to simulate everything would require a mesh with scales so small that it would require enormous amounts of memory and computing time which are not available today or any time soon in the future. In this thesis various methods to track particles in an unstructured grid are discussed an an early  version of the particle tracking algorithm \cite{macpherson08} is described. Also break up models and collision models are presented. Lagrangian particle tracking was introduced to OpenFOAM around the year 2000 with the introduction of \verb+dieselFoam+ which is described in this thesis.

\subsection{Porting Large Fortran Codebases to GPUs}
\label{sec:cudaPorting10}
This work deals with porting large legacy codes to CUDA \cite{portingCuda10}. It starts with a Fortran code called FEFLO which has around one million lines of code. OpenMP is used to parallelize the code for CPUs. Because porting all the code manually was just too much work a translator was written which ports the code automatically to CUDA. The existing OpenMP directives were used to generate CUDA kernels. The author claims that the work was done in a few months using a thousand line Python script based on FParser \cite{fparser}. It is further mentioned that memory transfers between the CPU and GPU should be avoided because they are slow and that the performance gain by just porting a few bottleneck routines will nullified by the additional time required for data transfers. Therefore memory transfers should happen at the beginning and at the end and not during the simulation. When discussing implementation details the author mentions thrust \cite{thrust} which was used for reductions. It is concluded that because of uniform coding conventions enforced during the development of FEFLO it was possible to port the code to a large percentage automatically to CUDA, just in a few cases rewriting code manually was necessary. The automatic code translator has a few limitations: Fine-grained parallelism must be already expressed in the code, which was done in this case with OpenMP. Additionally it only supports Fortran, C or C++ is not supported.
